{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbc0b90f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing functions for color space conversion and image saving from scikit-image\n",
    "from skimage.color import rgb2lab, lab2rgb, rgb2gray, xyz2lab, gray2rgb\n",
    "from skimage.io import imsave\n",
    "\n",
    "# Importing a metric for evaluating model performance from scikit-learn\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Importing image processing transforms from PyTorch\n",
    "from torchvision import transforms\n",
    "\n",
    "# Importing pre-trained models and model-related functionalities from torchvision and timm\n",
    "import torchvision.models as models\n",
    "import timm\n",
    "\n",
    "# Importing the Python Imaging Library (PIL) for image handling\n",
    "from PIL import Image\n",
    "\n",
    "# Importing NumPy for numerical operations\n",
    "import numpy as np\n",
    "\n",
    "# Importing operating system functions for file and directory operations\n",
    "import os\n",
    "\n",
    "# Importing the random module for generating random numbers\n",
    "import random\n",
    "\n",
    "# Importing PyTorch for deep learning functionalities\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "# Importing PyTorch's DataLoader and TensorDataset for handling data in batches\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# Importing additional image processing transforms from PyTorch\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a85e06e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get images\n",
    "X = []\n",
    "\n",
    "# Iterate over files in the 'Train/' directory\n",
    "for filename in os.listdir('Train/'):\n",
    "    # Open each image file, convert to RGB format, and append to the list\n",
    "    X.append(np.array(Image.open('Train/' + filename).convert('RGB')))\n",
    "\n",
    "# Convert the list of images to a NumPy array with dtype=float\n",
    "X = np.array(X, dtype=float)\n",
    "\n",
    "# Normalize pixel values to the range [0, 1]\n",
    "Xtrain = 1.0/255 * X\n",
    "\n",
    "# Load pre-trained Inception-ResNet-v2 model from torchvision\n",
    "# Note: 'pytorch/vision:v0.10.0' is the model zoo release version\n",
    "inception_resnet_v2 = torch.hub.load('pytorch/vision:v0.10.0', 'inception_v3', pretrained=True)\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "inception_resnet_v2.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acbaef00",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Encoder class definition\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Encoder, self).__init__()\n",
    "        # Convolutional layers for feature extraction\n",
    "        self.conv1 = nn.Conv2d(1, 64, kernel_size=3, stride=2, padding=1)\n",
    "        self.conv2 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(128, 128, kernel_size=3, stride=2, padding=1)\n",
    "        self.conv4 = nn.Conv2d(128, 256, kernel_size=3, padding=1)\n",
    "        self.conv5 = nn.Conv2d(256, 256, kernel_size=3, stride=2, padding=1)\n",
    "        self.conv6 = nn.Conv2d(256, 512, kernel_size=3, padding=1)\n",
    "        self.conv7 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
    "        self.conv8 = nn.Conv2d(512, 256, kernel_size=3, padding=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Forward pass through the encoder\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = F.relu(self.conv4(x))\n",
    "        x = F.relu(self.conv5(x))\n",
    "        x = F.relu(self.conv6(x))\n",
    "        x = F.relu(self.conv7(x))\n",
    "        x = F.relu(self.conv8(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "# Fusion class definition\n",
    "class Fusion(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Fusion, self).__init__()\n",
    "\n",
    "    def forward(self, x, embed_input):\n",
    "        # Reshape and repeat the embedding to match the feature map size\n",
    "        embed_input = embed_input.view(-1, 1000, 1, 1)\n",
    "        embed_input = embed_input.repeat(1, 1, x.size(2), x.size(3))\n",
    "        # Concatenate the features with the repeated embedding\n",
    "        x = torch.cat([x, embed_input], dim=1)\n",
    "        # Apply convolution for fusion\n",
    "        x = F.relu(nn.Conv2d(256 + 1000, 256, kernel_size=1)(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "# Decoder class definition\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Decoder, self).__init__()\n",
    "        # Convolutional and upsampling layers for decoding\n",
    "        self.conv1 = nn.Conv2d(256, 128, kernel_size=3, padding=1)\n",
    "        self.upsample1 = nn.Upsample(scale_factor=2, mode='nearest')\n",
    "        self.conv2 = nn.Conv2d(128, 64, kernel_size=3, padding=1)\n",
    "        self.upsample2 = nn.Upsample(scale_factor=2, mode='nearest')\n",
    "        self.conv3 = nn.Conv2d(64, 32, kernel_size=3, padding=1)\n",
    "        self.conv4 = nn.Conv2d(32, 16, kernel_size=3, padding=1)\n",
    "        self.conv5 = nn.Conv2d(16, 2, kernel_size=3, padding=1)\n",
    "        self.upsample3 = nn.Upsample(scale_factor=2, mode='nearest')\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Forward pass through the decoder\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.upsample1(x)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.upsample2(x)\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = F.relu(self.conv4(x))\n",
    "        x = F.tanh(self.conv5(x))\n",
    "        x = self.upsample3(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# ColorizationModel class definition\n",
    "class ColorizationModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ColorizationModel, self).__init__()\n",
    "        # Instantiate encoder, fusion, and decoder\n",
    "        self.encoder = Encoder()\n",
    "        self.fusion = Fusion()\n",
    "        self.decoder = Decoder()\n",
    "\n",
    "    def forward(self, encoder_input, embed_input):\n",
    "        # Forward pass through the entire colorization model\n",
    "        encoder_output = self.encoder(encoder_input)\n",
    "        fusion_output = self.fusion(encoder_output, embed_input)\n",
    "        decoder_output = self.decoder(fusion_output)\n",
    "        return decoder_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "541f19dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_batch(images):\n",
    "    # Define a series of image preprocessing transformations using transforms.Compose\n",
    "    preprocess = transforms.Compose([\n",
    "        transforms.ToPILImage(),  # Convert images to PIL Image\n",
    "        transforms.Resize((299, 299)),  # Resize images to (299, 299)\n",
    "        transforms.ToTensor(),  # Convert images to PyTorch tensors\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),  # Normalize image values\n",
    "    ])\n",
    "\n",
    "    # Apply the defined transformations to the input batch of images\n",
    "    return preprocess(images)\n",
    "\n",
    "def create_inception_embedding(grayscaled_rgb_resized):\n",
    "    # Preprocess the grayscaled RGB images\n",
    "    grayscaled_rgb_resized = preprocess_batch(grayscaled_rgb_resized)\n",
    "    \n",
    "    # Convert the preprocessed images to PyTorch tensor and adjust dimension order\n",
    "    input_tensor = torch.tensor(grayscaled_rgb_resized, dtype=torch.float32).permute(0, 3, 1, 2)\n",
    "    \n",
    "    # Disable gradient computation during inference\n",
    "    with torch.no_grad():\n",
    "        # Forward pass through the Inception-ResNet-v2 model to get embeddings\n",
    "        output = inception_resnet_v2(input_tensor)\n",
    "    \n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "995c05f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert data to PyTorch tensors\n",
    "lab_train = rgb2lab(Xtrain)\n",
    "X_train = lab_train[:, :, :, 0]  # Extract L channel (luminance)\n",
    "Y_train = lab_train[:, :, :, 1:] / 128  # Extract AB channels (chrominance) and normalize\n",
    "X_train = X_train.reshape(10, 256, 256, 1)  # Reshape to (batch_size, height, width, channels)\n",
    "Y_train = Y_train.reshape(10, 256, 256, 2)  # Reshape to (batch_size, height, width, channels)\n",
    "Xtrain_tensor = torch.tensor(X_train, dtype=torch.float32).permute(0, 3, 1, 2)  # Convert to PyTorch tensor, adjust dimension order\n",
    "Ytrain_tensor = torch.tensor(Y_train, dtype=torch.float32).permute(0, 3, 1, 2)  # Convert to PyTorch tensor, adjust dimension order\n",
    "\n",
    "# Instantiate the model and set up the optimizer and loss function\n",
    "model = ColorizationModel()  # Instantiate ColorizationModel\n",
    "criterion = nn.MSELoss()  # Mean Squared Error loss\n",
    "optimizer = optim.RMSprop(model.parameters(), lr=0.001, alpha=0.9)  # RMSprop optimizer\n",
    "\n",
    "# Data augmentation using torchvision.transforms\n",
    "transform = transforms.Compose([\n",
    "    transforms.RandomAffine(degrees=20, shear=[-5, 5], scale=(0.8, 1.2)),  # Random affine transformations\n",
    "    transforms.RandomHorizontalFlip(),  # Random horizontal flip\n",
    "    transforms.RandomVerticalFlip(),  # Random vertical flip\n",
    "    transforms.RandomRotation(20),  # Random rotation\n",
    "])\n",
    "\n",
    "# Define a custom dataset class for handling input data\n",
    "class ImageDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, X, embed, Y, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            X (list): Input data (grayscale images).\n",
    "            embed (list): Embeddings.\n",
    "            Y (list): Target data (colorized images).\n",
    "            transform (torchvision.transforms.Compose): Optional data transformations.\n",
    "        \"\"\"\n",
    "        self.X = X\n",
    "        self.Y = Y\n",
    "        self.embed = embed\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Returns:\n",
    "            int: Number of samples in the dataset.\n",
    "        \"\"\"\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            idx (int): Index of the sample.\n",
    "\n",
    "        Returns:\n",
    "            dict: Dictionary containing input image, embedding, and target data.\n",
    "        \"\"\"\n",
    "        sample = {'image': self.X[idx], 'embed': self.embed[idx], 'target': self.Y[idx]}\n",
    "\n",
    "        if self.transform:\n",
    "            # Apply transformations to the input image if specified\n",
    "            sample['image'] = self.transform(sample['image'])\n",
    "\n",
    "        return sample\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0757eb91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataLoader for batch training\n",
    "batch_size = 2\n",
    "\n",
    "# Convert RGB images to grayscale\n",
    "grayscaled_rgb = gray2rgb(rgb2gray(Xtrain))\n",
    "\n",
    "# Convert grayscaled images to PyTorch tensors and adjust dimension order\n",
    "grayscaled_rgb = torch.tensor(grayscaled_rgb, dtype=torch.float32).permute(0, 3, 1, 2)\n",
    "\n",
    "# Apply preprocessing transformations to the grayscaled images using preprocess_batch function\n",
    "transformed_batch = torch.stack([preprocess_batch(image) for image in grayscaled_rgb])\n",
    "\n",
    "# Compute embeddings using the pre-trained Inception ResNet V2 model\n",
    "with torch.no_grad():\n",
    "    embed = inception_resnet_v2(transformed_batch)\n",
    "\n",
    "# Create a custom dataset with grayscaled images, embeddings, and colorized images\n",
    "dataset = ImageDataset(Xtrain_tensor, embed, Ytrain_tensor, transform=transform)\n",
    "\n",
    "# Create DataLoader for batch training with the specified batch size and shuffle the data\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c2c4378",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "num_epochs = 100  # Number of training epochs\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0.0  # Initialize total loss for the current epoch\n",
    "    for batch in dataloader:\n",
    "        inputs, targets, em = batch['image'], batch['target'], batch['embed']  # Unpack input data, target data, and embeddings\n",
    "        optimizer.zero_grad()  # Zero the gradients to avoid accumulation\n",
    "        outputs = model(inputs, em)  # Forward pass: obtain model predictions\n",
    "        loss = criterion(outputs, targets)  # Calculate the loss between predictions and target\n",
    "        loss.backward()  # Backward pass: compute gradients\n",
    "        optimizer.step()  # Update model parameters using the optimizer\n",
    "        total_loss += loss.item()  # Accumulate the loss for the current batch\n",
    "\n",
    "    average_loss = total_loss / len(dataloader)  # Calculate the average loss for the epoch\n",
    "    print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {average_loss:.4f}\")  # Print the average loss for the current epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2451ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and preprocess test images\n",
    "color_me = []\n",
    "\n",
    "# Iterate through each file in the 'Test/' directory\n",
    "for filename in os.listdir('Test/'):\n",
    "    img = Image.open('Test/' + filename).convert('RGB')  # Open and convert the image to RGB format\n",
    "    img_array = np.array(img, dtype=float)  # Convert the image to a NumPy array\n",
    "    color_me.append(img_array)  # Append the NumPy array to the list\n",
    "\n",
    "color_me = np.array(color_me, dtype=float)  # Convert the list of NumPy arrays to a NumPy array\n",
    "x = color_me  # Save the color_me array for later use\n",
    "color_me = rgb2lab(1.0/255*color_me)[:, :, :, 0]  # Convert the RGB images to Lab color space and extract the L channel\n",
    "color_me = color_me.reshape(8, 256, 256, 1)  # Reshape to (batch_size, height, width, channels)\n",
    "\n",
    "# Convert numpy array to PyTorch tensor\n",
    "color_me_tensor = torch.tensor(color_me, dtype=torch.float32).permute(0, 3, 1, 2)  # Convert to PyTorch tensor, adjust dimension order\n",
    "\n",
    "# Convert RGB test images to PyTorch tensor and apply preprocessing transformations\n",
    "grayscaled_rgb_test = torch.tensor(x, dtype=torch.float32).permute(0, 3, 1, 2)\n",
    "transformed_test = torch.stack([preprocess_batch(image) for image in grayscaled_rgb_test])\n",
    "\n",
    "# Compute embeddings for test images using the pre-trained Inception ResNet V2 model\n",
    "with torch.no_grad():\n",
    "    embed_test = inception_resnet_v2(transformed_test)\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Test the model\n",
    "with torch.no_grad():\n",
    "    output = model(color_me_tensor, embed_test)  # Forward pass to obtain colorized images\n",
    "    output = output.cpu().numpy()  # Convert PyTorch tensor to numpy array\n",
    "\n",
    "# Rescale the output to the original Lab color space range\n",
    "output = output * 128\n",
    "\n",
    "# Output colorizations\n",
    "for i in range(len(output)):\n",
    "    cur = np.zeros((256, 256, 3))\n",
    "    cur[:, :, 0] = color_me[i][:, :, 0]  # Copy the L channel to the output\n",
    "    cur[:, :, 1:] = output[i].transpose(1, 2, 0)  # Copy the colorized AB channels to the output\n",
    "    output_img = lab2rgb(cur)  # Convert Lab back to RGB\n",
    "    output_img = (output_img * 255).astype(np.uint8)  # Rescale pixel values to 0-255\n",
    "    # Save the output image\n",
    "    imsave(\"result/img_\"+str(i)+\".png\", output_img)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
